<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="文本分类器,微调"><meta name="description" content="文本预处理、训练文本分类器、微调模型"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="baidu-site-verification" content="codeva-fVFbpHJJO8"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer"><title>NLP文本分类器 | SQX BLOG</title><link rel="icon" type="image/png" href="../../../../favicon.png"><link rel="stylesheet" type="text/css" href="../../../../libs/awesome/css/all.min.css"><link rel="stylesheet" type="text/css" href="../../../../libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="../../../../libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="../../../../libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="../../../../libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/matery.css"><link rel="stylesheet" type="text/css" href="../../../../css/my.css"><link rel="stylesheet" type="text/css" href="../../../../css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="../../../../libs/tocbot/tocbot.css"><link rel="stylesheet" href="../../../../css/post.css"><link rel="stylesheet" type="text/css" href="../../../../css/reward.css"><script src="../../../../libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 6.3.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="../../../../index.html" class="waves-effect waves-light"><div><img src="/themes/matery/source/medias/loading.gif" data-original="../../../../medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">SQX BLOG</span></div></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="../../../../index.html" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>归档</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../friends" class="waves-effect waves-light"><i class="fas fa-address-book" style="zoom:.6"></i> <span>友情链接</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/themes/matery/source/medias/loading.gif" data-original="../../../../medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">SQX BLOG</div><div class="logo-desc">Technical writer | Life adventurer</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="../../../../index.html" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="../../../../tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="../../../../categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="../../../../archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="../../../../about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="../../../../friends" class="waves-effect waves-light"><i class="fa-fw fas fa-address-book"></i> 友情链接</a></li></ul></div></div></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url(../../../medias/featureimages/30.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">NLP文本分类器</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="../../../../tags/nlp/"><span class="chip bg-color">nlp</span> </a><a href="../../../../tags/%E9%A2%84%E8%AE%AD%E7%BB%83/"><span class="chip bg-color">预训练</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="../../../../categories/NLP/" class="post-category">NLP</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2024-06-03</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 8.6k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 32 分</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp; <span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><link rel="stylesheet" href="../../../../libs/prism/prism.min.css"><div class="card-content article-card-content"><div id="articleContent"><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406030947366.png" width="600"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">通用语言模型微调</div></center><p>在维基百科上训练的分类模型，通过微调语言模型（维基百科和IMDb语料库的风格不同），迁移到IMDb数据集的情感分类模型中。</p><p>自监督学习：使用嵌入在自变量中的标签来训练模型，而不需要外部标签。例如，训练模型预测文本中的下一个单词。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> fastbook
fastbook<span class="token punctuation">.</span>setup_book<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">from</span> fastbook <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> IPython<span class="token punctuation">.</span>display <span class="token keyword">import</span> display<span class="token punctuation">,</span>HTML<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在自然语言处理（NLP）的迁移学习中，<strong>通用语言模型微调（ULMFit）</strong>方法是一个非常有效的三阶段过程：</p><ol><li><strong>预训练语言模型</strong>：首先，我们从一个大型语料库（如维基百科）预训练一个语言模型。这个模型学习了语言的基本结构和词汇。</li><li><strong>领域特定微调</strong>：然后，我们将预训练的语言模型在目标任务的相关语料库上进行微调。例如，如果我们的目标是IMDb电影评论分类，我们会在IMDb的评论数据集上进行微调，这样模型就能适应那里的语言风格和专有名词。</li><li><strong>分类器微调</strong>：最后，我们在微调后的语言模型的基础上，进一步训练一个分类器来执行特定的任务，比如情感分析。</li></ol><p>这个过程的关键在于，通过在目标任务的语料库上进行微调，语言模型能够更好地理解和生成与任务相关的文本。这种方法已经在多个NLP任务中显示出了显著的性能提升。</p><blockquote><p>Universal Language Model Fine-tuning for Text Classification <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.06146">https://arxiv.org/abs/1801.06146</a> (ACL-2018)</p></blockquote><h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><p>如何使用神经网络来预测一个句子中的下一个单词？</p><p>在构建语言模型时，我们面临的一个挑战是处理不同长度的句子和可能非常长的文档。为了预测句子中的下一个词，我们可以采用以下步骤：</p><ol><li><strong>构建词汇表</strong>：将数据集中的所有文档连接成一个长字符串，并将其分割成单词（或语义单元token），形成一个非常长的单词列表（或“词汇表vocab”）。</li><li><strong>索引替换</strong>：将每个单词替换为其在词汇表中的索引。</li><li><strong>创建嵌入矩阵</strong>：为词汇表中的每个单词创建一个嵌入向量，并将这些向量组成一个嵌入矩阵。</li><li><strong>使用嵌入矩阵</strong>：将嵌入矩阵作为神经网络的第一层。嵌入矩阵可以直接接受步骤2中创建的原始词汇索引作为输入。这与把代表索引的独热编码向量作为输入矩阵等效，但速度更快、效率更高。</li></ol><p>在处理文本时，我们需要考虑序列的概念。我们的自变量将是从第一个单词开始到倒数第二个单词结束的单词序列，而因变量将是从第二个单词开始到最后一个单词结束的单词序列。</p><p>我们的词汇表将包含预训练模型中已有的常见单词和特定于我们语料库的新单词（例如电影术语或演员姓名）。我们的嵌入矩阵将相应地构建：对于预训练模型词汇表中的单词，我们将使用预训练模型嵌入矩阵中的相应行；但对于新单词，我们没有预先训练的嵌入向量，因此我们将用随机向量初始化相应的行。</p><p>通过这种方式，我们的神经网络能够学习预测给定序列中下一个单词的能力，这是构建语言模型的基础。</p><p>在自然语言处理（NLP）中创建语言模型涉及一系列步骤，每个步骤都有其专业术语，并且fastai和PyTorch提供了相应的类来帮助实现。以下是这些步骤的概要：</p><ol><li><strong>分词（Tokenization）</strong>：将文本转换为单词（或字符、子字符串）列表，这取决于模型的粒度。</li><li><strong>数值化（Numericalization）</strong>：创建一个包含所有唯一单词（词汇表）的列表，并通过查找其在词汇表中的索引，将每个单词转换为一个数字。</li><li><strong>语言模型数据加载器创建（Language model data loader creation）</strong>：fastai提供了一个<code>LMDataLoader</code>类，它自动处理创建一个从自变量偏移一个单位的因变量。它还处理了一些重要细节，例如如何以保持因变量和自变量所需结构的方式来把数据集洗乱。</li><li><strong>语言模型创建（Language model creation）</strong>：我们需要一种特殊的模型来处理任意大小的输入列表。有多种方法可以实现这一点；在这里，将使用递归神经网络（RNN）。</li></ol><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>“将文本转换成单词列表”时，如何处理标点符号？如何处理“don’t”？如何处理有连字符的词？如何处理长长的医学或化学词汇？</p><ul><li><p><strong>基于单词</strong></p><p>用空格拆分句子，并应用特定的语言规则，即使在没有空格的情况下，也要尝试分离部分意义（例如将 “don’t” 分割为 “do n’t”）。通常，标点符号也被分割成独立的单元。</p></li><li><p><strong>基于子词</strong></p><p>根据最常出现的子字符串将单词拆分成更小的部分。例如，“occasion” 可能被分词为 “o c ca sion”。</p></li><li><p><strong>基于字符</strong></p><p>将句子分成几个单独的字符。</p></li></ul><p><code>语义单元token</code>：由分词过程创建的列表中的一个元素。它可以是一个单词、一个单词的一部分（子词），或单个字符。</p><h2 id="用fastai单词分词"><a href="#用fastai单词分词" class="headerlink" title="用fastai单词分词"></a>用fastai单词分词</h2><p>使用IMDb数据集</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span><span class="token builtin">all</span> <span class="token keyword">import</span> <span class="token operator">*</span>
path <span class="token operator">=</span> untar_data<span class="token punctuation">(</span>URLs<span class="token punctuation">.</span>IMDB<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>使用get_text_files获取文本文件</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">files <span class="token operator">=</span> get_text_files<span class="token punctuation">(</span>path<span class="token punctuation">,</span> folders <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token string">'unsup'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
txt <span class="token operator">=</span> files<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
txt<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">75</span><span class="token punctuation">]</span> <span class="token comment"># 对一篇评论分词，只显示部分</span>
<span class="token comment"># 'I love a good sappy love story (and I\'m a guy) but when I rented "Love Stor'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>分词器分词</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">spacy <span class="token operator">=</span> WordTokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
toks <span class="token operator">=</span> first<span class="token punctuation">(</span>spacy<span class="token punctuation">(</span><span class="token punctuation">[</span>txt<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coll_repr<span class="token punctuation">(</span>toks<span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token comment">#144) ['I','love','a','good','sappy','love','story','(','and','I',"'m",'a','guy',')','but','when','I','rented','"','Love','Story','"','I','prayed','for','the','end','to','come','as'...]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><code>coll_repr(collection, n)</code>函数显示结果，可以显示collection的前n项，不含标点。可以看出是按标点符号分开的，<code>he's</code>被分成<code>he</code>和<code>'s</code>。那一句话结束的<code>.</code>和句中的<code>.</code>，spaCy也做了如下处理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">first<span class="token punctuation">(</span>spacy<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'The U.S. dollar $1 is $1.00.'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token comment">#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>使用了 <code>spacy</code> 库对一段文本进行了处理。<code>spacy</code> 是一个用于自然语言处理的 Python 库，它可以用于词性标注、命名实体识别、依存关系解析等任务。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">tkn <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>spacy<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coll_repr<span class="token punctuation">(</span>tkn<span class="token punctuation">(</span>txt<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token comment">#157) ['xxbos','i','love','a','good','sappy','love','story','(','and','xxmaj','i',"'m",'a','guy',')','but','when','i','rented','"','love','xxmaj','story','"','i','prayed','for','the','end','to'...]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>fastai使用<code>Tokenizer</code>类为分词过程添加了一些附加功能：</p><p>如上有一些以“xx”开头的语义单词，这在英语中不是一个常见的单词前缀，而是特殊的语义单元。例如，列表第一项，xxbos，表示一个新文本的开始（“BOS”是一个标准的NLP缩写“beginning of stream”，意味着“流的开始”）。通过识别这个定义为开始的语义单元，该模型能够明白它需要“忘记”之前说过的话，并专注于即将到来的单词。有助于模型在处理连续文本流时，能够区分不同文本之间的界限。</p><p>从某种意义上说，这些规则旨在使模型更容易识别句子的重要部分。将原始的英语语言序列翻译成简化的分词语言（易于模型学习）。</p><p>在自然语言处理中，特殊的标记化规则可以帮助模型更有效地学习和表示文本数据。例如：</p><ul><li><strong>重复字符的处理</strong>：如果一个句子中有连续的四个感叹号（“!!!”），规则会将其替换为一个特殊的重复字符标记，后面跟着数字4，然后是一个单独的感叹号。这样，模型的嵌入矩阵就可以编码关于重复标点的一般概念，而不是为每个标点的每个重复次数都需要一个单独的标记。</li><li><strong>大写字母的处理</strong>：一个大写的单词会被替换为一个特殊的大写标记，后面跟着这个单词的小写版本。这样，嵌入矩阵只需要单词的小写版本，从而节省了计算和内存资源，但模型仍然可以学习到大写的概念。</li></ul><p>这些规则使得模型能够以更紧凑的形式学习文本的特征，同时保留了文本的重要语义信息，如强调和命名实体的大写形式。</p><blockquote><p>一些特殊语义单元</p><p>xxbos 表示一个文本的开始</p><p>xxmsj 表示下一个单词以大写开头</p><p>xxunk 表示下一个单词未知</p></blockquote><p>要查看所使用的规则，可以检查默认规则：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">defaults<span class="token punctuation">.</span>text_proc_rules
<span class="token comment">### </span>
<span class="token punctuation">[</span><span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>fix_html<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
 <span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>replace_rep<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
 <span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>replace_wrep<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
 <span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>spec_add_spaces<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
 <span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>rm_useless_spaces<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
 <span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>replace_all_caps<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
 <span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>replace_maj<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
 <span class="token operator">&lt;</span>function fastai<span class="token punctuation">.</span>text<span class="token punctuation">.</span>core<span class="token punctuation">.</span>lowercase<span class="token punctuation">(</span>t<span class="token punctuation">,</span> add_bos<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> add_eos<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以下是对每种功能的简要概述：</p><ul><li><strong>fix_html</strong>：将特殊的HTML字符替换为可读版本（IMDb评论中有很多这样的字符）。</li><li><strong>replace_rep</strong>：将重复三次及以上的任何字符替换为重复的特殊语义单元（xxrep），后接重复的次数，然后是该字符。</li><li><strong>replace_wrep</strong>：将重复三次及以上的任何单词替换为单词重复的特殊语义单元（xxwrep），后接重复的次数，然后是该单词。</li><li><strong>spec_add_spaces</strong>：在/和#前后添加空格。</li><li><strong>rm_useless_spaces</strong>：删除所有重复的空格字符。</li><li><strong>replace_all_caps</strong>：将全部大写的单词转换为小写，并在其前面添加一个表示全部大写的特殊语义单元（xxup）。</li><li><strong>replace_maj</strong>：将首字母大写的单词转换为小写，并在其前面添加一个表示首字母大写的特殊语义单元（xxmaj）。</li><li><strong>lowercase</strong>：将所有文本转换为小写，并在开始处（xxbos）和/或结束处（xxeos）添加特殊标记。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">coll_repr<span class="token punctuation">(</span>tkn<span class="token punctuation">(</span><span class="token string">'&amp;copy;   Fast.ai www.fast.ai/INDEX'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">)</span> <span class="token comment">#最大只显示31个元素</span>
<span class="token string">"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="根据子词分词"><a href="#根据子词分词" class="headerlink" title="根据子词分词"></a>根据子词分词</h2><p>子词分词方法是处理像中文和日文这样的无空格语言的有效方式。这种方法不依赖于空格来分隔意义，而是通过识别语料库中常见的字母组合来构建词汇表。然后，使用这个词汇表来分词。过程：</p><ol><li><strong>分析语料库</strong>：检查一定数量的文档，找出最常出现的字母组合。例如，如果我们分析2000条电影评论，我们可能会发现“电影”、“好看”、“演技”等字词组合频繁出现。</li><li><strong>使用词汇表分词</strong>：根据步骤1中创建的词汇表，将文本分解为子词单元。例如，“我的名字是郝杰瑞”可能会被分解为“我的”、“名字”、“是”、“郝”、“杰”、“瑞”。</li></ol><p>这种方法特别适用于处理那些词汇创造性很强、新词频繁出现的语言，因为它允许模型通过已知的子词单元来理解和生成未曾见过的词汇。这也有助于机器学习模型更好地处理语言的复杂性和多样性。</p><p>语料库使用前2000条影评：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">txts <span class="token operator">=</span> L<span class="token punctuation">(</span>o<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> o <span class="token keyword">in</span> files<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>实例化分词器</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">## 文本中有'gbk'编码无法处理的字符</span>
<span class="token keyword">def</span> <span class="token function">clean_text</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> text<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'gbk'</span><span class="token punctuation">,</span> <span class="token string">'ignore'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'gbk'</span><span class="token punctuation">)</span>

txts <span class="token operator">=</span> L<span class="token punctuation">(</span>clean_text<span class="token punctuation">(</span>o<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> o <span class="token keyword">in</span> files<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">subword</span><span class="token punctuation">(</span>sz<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sp <span class="token operator">=</span> SubwordTokenizer<span class="token punctuation">(</span>vocab_sz<span class="token operator">=</span>sz<span class="token punctuation">)</span>
    sp<span class="token punctuation">.</span>setup<span class="token punctuation">(</span>txts<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>first<span class="token punctuation">(</span>sp<span class="token punctuation">(</span><span class="token punctuation">[</span>txt<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

subword<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span>
<span class="token comment"># '▁I ▁love ▁a ▁good ▁sa pp y ▁love ▁story ▁( and ▁I \' m ▁a ▁guy ) ▁but ▁when ▁I ▁r ent ed ▁" L o ve ▁St or y " ▁I ▁p ra y ed ▁for ▁the ▁end ▁to'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用fastai中的子词分词器时，特殊字符<code>_</code>代表原文中的空格字符。</p><p>在subword函数中，<code>sz</code> 是一个形参，它代表了想创建的词汇表的大小。这个参数将被传递给 <code>SubwordTokenizer</code>，用于确定在分词时应该使用的最大词汇表大小。</p><p><code>SubwordTokenizer</code> 是一个分词器，它可以将文本分解为子词单元。子词单元是介于单词和字符之间的文本单元，它们可以更好地处理稀有词和词根变化。</p><p><code>vocab_sz</code> 参数决定了分词器在分解文本时可以使用的子词单元的最大数量。如果 <code>vocab_sz</code> 较大，那么分词器可以使用更多的子词单元，这可能会导致更好的模型性能，但也可能会增加模型的复杂性和训练时间。</p><p><code>sp.setup(txts)</code> 这行代码是在训练分词器，使其学习如何将文本分解为子词单元。<code>txts</code> 应该是一个包含大量文本的列表，分词器将从这些文本中学习子词单元。</p><p><code>' '.join(first(sp([txt]))[:40])</code> 这行代码是在使用训练好的分词器将文本分解为子词单元，然后取出前40个子词单元，并将它们连接成一个字符串。<code>txt</code> 应该是一个字符串，代表要被分词的文本。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">subword<span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span> <span class="token comment"># 更小的vocab，每个语义单元含更少的字符，需更多语义单元表示一个句子</span>
<span class="token comment"># "▁I ▁lo ve ▁a ▁ g o o d ▁ s a p p y ▁lo ve ▁st or y ▁ ( an d ▁I ' m ▁a ▁ g u y ) ▁but ▁w h en ▁I ▁ r"</span>
subword<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span> <span class="token comment"># 更大的vocab，那大多数单词将出现在vocab中，训练快，但是嵌入矩阵大，需更多数据学习</span>
<span class="token comment"># '▁I ▁love ▁a ▁good ▁ s appy ▁love ▁story ▁( and ▁I \' m ▁a ▁guy ) ▁but ▁when ▁I ▁rented ▁" Lo ve ▁Story " ▁I ▁pray ed ▁for ▁the ▁end ▁to ▁come ▁as ▁quickly ▁and ▁pain less ly'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>子词分词器是在字符分词和词分词之间的扩展，无需开发语言特定的算法，强大！！！</p><h2 id="fastai数值化"><a href="#fastai数值化" class="headerlink" title="fastai数值化"></a>fastai数值化</h2><p>根据上述单词分词，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">toks <span class="token operator">=</span> tkn<span class="token punctuation">(</span>txt<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coll_repr<span class="token punctuation">(</span>tkn<span class="token punctuation">(</span>txt<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># (#157) ['xxbos','i','love','a','good','sappy','love','story','(','and','xxmaj','i',"'m",'a','guy',')','but','when','i','rented','"','love','xxmaj','story','"','i','prayed','for','the','end','to'...]</span>
toks200 <span class="token operator">=</span> txts<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tkn<span class="token punctuation">)</span> <span class="token comment"># 小子集实验</span>
toks200<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token comment"># (#157) ['xxbos','i','love','a','good','sappy','love','story','(','and'...]</span>
num <span class="token operator">=</span> Numericalize<span class="token punctuation">(</span><span class="token punctuation">)</span>
num<span class="token punctuation">.</span>setup<span class="token punctuation">(</span>toks200<span class="token punctuation">)</span>
coll_repr<span class="token punctuation">(</span>num<span class="token punctuation">.</span>vocab<span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span>
<span class="token comment"># "(#2112) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','it','i','in'...]"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在fastai库中，<code>Numericalize</code> 是一个将文本转换为数字的过程，这对于准备数据以供模型训练非常重要。这里是如何使用 <code>Numericalize</code> 的一些关键点：</p><ol><li><strong>特殊规则标记</strong>：在词汇表的开始部分，会有一些特殊的标记，如 <code>xxunk</code>（未知词），<code>xxpad</code>（填充词）等，这些用于处理文本中的特殊情况。</li><li><strong>频率排序</strong>：之后，每个单词根据其在语料库中出现的频率被添加到词汇表中，最常见的词汇排在前面。</li><li><strong>参数设置</strong>：<ul><li><code>min_freq</code>：这个参数决定了一个单词必须在语料库中出现的最小次数才能被包含在词汇表中。默认值为3，意味着出现次数少于3次的单词会被替换为 <code>xxunk</code>。</li><li><code>max_vocab</code>：这个参数限制了词汇表的最大大小。默认值为60000，表示只有出现频率最高的60000个单词会被保留，其他的会被替换为 <code>xxunk</code>。</li></ul></li><li><strong>使用自定义词汇表</strong>：如果你有一个预先定义好的词汇表，你可以通过 <code>vocab</code> 参数将其传递给 <code>Numericalize</code> 对象，这样就可以根据你的词汇表来数值化数据集。</li><li><strong>数值化对象的使用</strong>：创建 <code>Numericalize</code> 对象后，你可以像使用函数一样调用它，将文本转换为数字序列。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">nums <span class="token operator">=</span> num<span class="token punctuation">(</span>toks<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">;</span> nums
<span class="token comment"># TensorText([   2,   18,  171,   12,   74,    0,  171,  102,   40,   13,    8,   18,  157,   12,  210,   37,   28,   65,   18, 1561])</span>
<span class="token comment"># 是否可以映射回源文本</span>
<span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>num<span class="token punctuation">.</span>vocab<span class="token punctuation">[</span>o<span class="token punctuation">]</span> <span class="token keyword">for</span> o <span class="token keyword">in</span> nums<span class="token punctuation">)</span>
<span class="token comment"># "xxbos i love a good xxunk love story ( and xxmaj i 'm a guy ) but when i rented"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="将文本分批作为语言模型的输入"><a href="#将文本分批作为语言模型的输入" class="headerlink" title="将文本分批作为语言模型的输入"></a>将文本分批作为语言模型的输入</h2><p>不同于图像，将张量调整为相同大小然后堆叠，语言模型要按顺序阅读文本才能有效地预测下一个单词。每一个新批次都应该从上一个批次停止的地方开始。</p><p>假设有以下文本：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">In this chapter<span class="token punctuation">,</span> we will go back over the example of classifying movie reviews we studied <span class="token keyword">in</span> chapter <span class="token number">1</span> <span class="token keyword">and</span> dig deeper under the surface<span class="token punctuation">.</span> First we will look at the processing steps necessary to convert text into numbers <span class="token keyword">and</span> how to customize it<span class="token punctuation">.</span> By doing this<span class="token punctuation">,</span> we'll have another example of the PreProcessor used <span class="token keyword">in</span> the data block API<span class="token punctuation">.</span>\nThen we will study how we build a language model <span class="token keyword">and</span> train it <span class="token keyword">for</span> a <span class="token keyword">while</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>单词分词</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">stream <span class="token operator">=</span> <span class="token string">"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\nThen we will study how we build a language model and train it for a while."</span>
tokens <span class="token operator">=</span> tkn<span class="token punctuation">(</span>stream<span class="token punctuation">)</span>
bs<span class="token punctuation">,</span>seq_len <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">,</span><span class="token number">15</span>
d_tokens <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>tokens<span class="token punctuation">[</span>i<span class="token operator">*</span>seq_len<span class="token punctuation">:</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>seq_len<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>d_tokens<span class="token punctuation">)</span>
display<span class="token punctuation">(</span>HTML<span class="token punctuation">(</span>df<span class="token punctuation">.</span>to_html<span class="token punctuation">(</span>index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>header<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202405311059331.png" width="900"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">分词表</div></center><ol><li><strong>选择序列长度</strong>：假设我们选择序列长度为5，这意味着我们将数据分割成长度为5的子数组。</li><li><strong>创建子数组</strong>：从原始数组的开始，我们按顺序取出长度为5的片段，直到覆盖整个数组。</li><li><strong>保持顺序</strong>：在处理这些子数组时，我们需要保持它们的顺序，因为模型会依赖这个顺序来预测下一个标记。</li></ol><p>每个子数组都会依次输入到模型中。这样，模型可以在处理当前子数组时，记住之前子数组的信息，从而更好地预测接下来的标记。</p><p>这种方法允许模型有效地处理大规模数据集，同时保持了数据的顺序性和上下文信息，这对于文本生成和其他序列预测任务非常重要。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">bs<span class="token punctuation">,</span>seq_len <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">,</span><span class="token number">5</span>
d_tokens <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>tokens<span class="token punctuation">[</span>i<span class="token operator">*</span><span class="token number">15</span><span class="token punctuation">:</span>i<span class="token operator">*</span><span class="token number">15</span><span class="token operator">+</span>seq_len<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>d_tokens<span class="token punctuation">)</span>
display<span class="token punctuation">(</span>HTML<span class="token punctuation">(</span>df<span class="token punctuation">.</span>to_html<span class="token punctuation">(</span>index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>header<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202405311102265.png" width="400"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">第一个子数组</div></center><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 第二、第三个</span>
bs<span class="token punctuation">,</span>seq_len <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">,</span><span class="token number">5</span>
d_tokens <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>tokens<span class="token punctuation">[</span>i<span class="token operator">*</span><span class="token number">15</span><span class="token operator">+</span>seq_len<span class="token punctuation">:</span>i<span class="token operator">*</span><span class="token number">15</span><span class="token operator">+</span><span class="token number">2</span><span class="token operator">*</span>seq_len<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>d_tokens<span class="token punctuation">)</span>
display<span class="token punctuation">(</span>HTML<span class="token punctuation">(</span>df<span class="token punctuation">.</span>to_html<span class="token punctuation">(</span>index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>header<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

bs<span class="token punctuation">,</span>seq_len <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">,</span><span class="token number">5</span>
d_tokens <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>tokens<span class="token punctuation">[</span>i<span class="token operator">*</span><span class="token number">15</span><span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">:</span>i<span class="token operator">*</span><span class="token number">15</span><span class="token operator">+</span><span class="token number">15</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>d_tokens<span class="token punctuation">)</span>
display<span class="token punctuation">(</span>HTML<span class="token punctuation">(</span>df<span class="token punctuation">.</span>to_html<span class="token punctuation">(</span>index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>header<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在处理电影评论数据集时，将文本转换为连续的流是一个重要的步骤。这里是整个过程的详细解释：</p><ol><li><strong>文本串联</strong>：首先，我们将所有文本串联起来形成一个长文本流。这就像将所有评论拼接成一篇超长的文章。</li><li><strong>随机化顺序</strong>：为了让模型不依赖于特定的文本顺序，我们在每个训练周期（epoch）开始时随机打乱文档的顺序。这样做可以提高模型的泛化能力。</li><li><strong>分割成批次</strong>：然后，我们将这个长文本流分割成多个批次。如果我们有50,000个标记，并且设置批次大小为10，那么我们将得到10个包含5,000个标记的小流。</li><li><strong>保持标记顺序</strong>：在分割时，我们保持标记的顺序不变。例如，第一个小流包含标记1到5,000，第二个小流包含标记5,001到10,000，以此类推。这样做是为了让模型能够连续地阅读文本。</li><li><strong>添加特殊标记</strong>：在预处理期间，我们在每个新条目的开始添加一个 <code>xxbos</code> 标记，以便模型知道何时开始阅读新的文本条目。</li><li><strong>模型的内部状态</strong>：由于模型具有内部状态，它可以记住之前读取的内容，因此无论我们选择的序列长度如何，它都能产生相同的激活。</li><li><strong>fastai库的自动化</strong>：当我们使用fastai库创建一个 <code>LMDataLoader</code> 时，上述所有步骤都在幕后自动完成。我们首先将 <code>Numericalize</code> 对象应用于分词后的文本，以将其转换为数字序列。</li></ol><p>这个过程确保了模型在训练时能够处理大量的文本数据，同时保持了文本的结构和上下文信息。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nums200 <span class="token operator">=</span> toks200<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>num<span class="token punctuation">)</span>
dl <span class="token operator">=</span> LMDataLoader<span class="token punctuation">(</span>nums200<span class="token punctuation">)</span>
x<span class="token punctuation">,</span>y <span class="token operator">=</span> first<span class="token punctuation">(</span>dl<span class="token punctuation">)</span>
x<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape
<span class="token comment"># (torch.Size([64, 72]), torch.Size([64, 72]))</span>
<span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>num<span class="token punctuation">.</span>vocab<span class="token punctuation">[</span>o<span class="token punctuation">]</span> <span class="token keyword">for</span> o <span class="token keyword">in</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#自变量的第一行</span>
<span class="token comment"># "xxbos i love a good xxunk love story ( and xxmaj i 'm a guy ) but when i rented"</span>
<span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>num<span class="token punctuation">.</span>vocab<span class="token punctuation">[</span>o<span class="token punctuation">]</span> <span class="token keyword">for</span> o <span class="token keyword">in</span> y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#因变量的第一行，自变量偏移一个单位</span>
<span class="token comment"># 'i love a good xxunk love story ( and xxmaj i \'m a guy ) but when i rented "'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="训练文本分类器"><a href="#训练文本分类器" class="headerlink" title="训练文本分类器"></a>训练文本分类器</h1><p>在使用迁移学习训练先进的文本分类器时，确实有两个主要步骤：</p><ol><li><strong>微调语言模型</strong>：首先，我们需要将预训练在维基百科上的语言模型微调到IMDb评论的语料库上。这意味着我们要让模型适应IMDb评论的特定语言风格和用词。</li><li><strong>训练分类器</strong>：一旦语言模型被微调，我们就可以使用它来训练一个分类器，该分类器将能够根据评论的内容预测电影评价是正面还是负面。</li></ol><h3 id="使用数据块训练语言模型"><a href="#使用数据块训练语言模型" class="headerlink" title="使用数据块训练语言模型"></a>使用数据块训练语言模型</h3><p>在fastai中，当<code>TextBlock</code>被传递给<code>DataBlock</code>时，会自动处理**分词(tokenization)和数值化(numericalization)**。你可以将传递给<code>Tokenize</code>和<code>Numericalize</code>的所有参数也传递给<code>TextBlock</code>。不要忘记<code>DataBlock</code>的<code>summary</code>方法，它对于调试数据问题非常有用。</p><p>下面是我们如何使用<code>TextBlock</code>来创建一个语言模型，使用fastai的默认设置：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">get_imdb <span class="token operator">=</span> partial<span class="token punctuation">(</span>get_text_files<span class="token punctuation">,</span> folders<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token string">'unsup'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

dls_lm <span class="token operator">=</span> DataBlock<span class="token punctuation">(</span>
    blocks<span class="token operator">=</span>TextBlock<span class="token punctuation">.</span>from_folder<span class="token punctuation">(</span>path<span class="token punctuation">,</span> is_lm<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    get_items<span class="token operator">=</span>get_imdb<span class="token punctuation">,</span> splitter<span class="token operator">=</span>RandomSplitter<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">.</span>dataloaders<span class="token punctuation">(</span>path<span class="token punctuation">,</span> path<span class="token operator">=</span>path<span class="token punctuation">,</span> bs<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>这段代码中的 <code>TextBlock.from_folder(path, is_lm=True)</code> 是在创建一个 <code>TextBlock</code> 对象，这是 <code>fastai</code> 库中用于处理文本数据的一个类。</p><p><code>from_folder(path, is_lm=True)</code> 是 <code>TextBlock</code> 类的一个类方法，它从指定的文件夹中加载文本数据。<code>path</code> 参数是你的数据的路径。<code>is_lm=True</code> 参数表示这个 <code>TextBlock</code> 是用于语言模型的。语言模型是一种预测下一个词的模型，所以它需要看到所有的词，而不只是标签。当 <code>is_lm=True</code> 时，<code>TextBlock</code> 会将所有的文本数据视为一个连续的文本流，而不是分割成单独的样本。</p></blockquote><p>在<code>DataBlock</code>中使用<code>TextBlock</code>的一个不同之处是，我们不是直接使用类（即<code>TextBlock(...)</code>），而是调用一个类方法。<code>TextBlock</code>之所以特殊，是因为设置数值化器的词汇表可能需要很长时间（我们必须读取并分词每个文档来获取词汇表）。为了尽可能高效，它执行了一些优化：</p><ul><li>它在一个临时文件夹中保存分词后的文档，这样就不必多次分词。</li><li>它并行运行多个分词过程，以利用计算机的CPU。</li></ul><p>我们需要告诉<code>TextBlock</code>如何访问文本，以便它可以进行这个初始预处理——这就是<code>from_folder</code>的作用。</p><p>然后，<code>show_batch</code>以通常的方式工作。这意味着它会显示数据批次的样本，这对于检查数据是否正确加载和处理非常有用。这是一个调试数据问题时的重要步骤，因为它可以让你直观地看到数据的实际外观，从而更容易发现潜在的问题。</p><p>总的来说，<code>TextBlock</code>通过类方法提供了一个高效的方式来处理文本数据，确保在创建语言模型或其他自然语言处理任务时，数据预处理既快速又准确。这种方法的优势在于它减少了重复工作，并通过并行处理最大化了资源利用率。这些特性使得<code>fastai</code>成为处理大规模文本数据集的强大工具。(Windows上不可以多进程并行处理，所以代码中这样设置num_workers=0)</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dls_lm<span class="token punctuation">.</span>show_batch<span class="token punctuation">(</span>max_n<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406021620062.png" width="600"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">预处理后的数据</div></center><h3 id="微调语言模型"><a href="#微调语言模型" class="headerlink" title="微调语言模型"></a>微调语言模型</h3><p>在神经网络中，我们将使用嵌入（embeddings）来将整数词索引转换为激活值，这与我们在协同过滤和表格建模中所做的类似。然后，我们将这些嵌入输入到一个循环神经网络（RNN）中，使用一种称为AWD-LSTM的架构。预训练模型中的嵌入会与为预训练词汇表中不存在的词添加的随机嵌入合并。这一过程在<code>language_model_learner</code>中自动处理。</p><p>简单来说，嵌入是一种将词汇映射到高维空间中的向量的技术，这些向量能够捕捉到词汇的语义信息。在预训练的语言模型中，这些嵌入已经学习到了大量的语言结构和单词之间的关系。当我们在模型中加入新词时，我们会创建随机的嵌入向量，并在训练过程中逐渐调整它们，使其与预训练的嵌入相融合。</p><p>AWD-LSTM是一种特殊的RNN架构，它通过使用Dropout技术来避免过拟合，同时保持了模型的复杂性和表达能力。在<code>language_model_learner</code>中，AWD-LSTM模型可以自动处理新词的嵌入，并将它们与预训练的嵌入合并，从而使模型能够有效地处理新的文本数据。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn <span class="token operator">=</span> language_model_learner<span class="token punctuation">(</span>
    dls_lm<span class="token punctuation">,</span> AWD_LSTM<span class="token punctuation">,</span> drop_mult<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> 
    metrics<span class="token operator">=</span><span class="token punctuation">[</span>accuracy<span class="token punctuation">,</span> Perplexity<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to_fp16<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>创建一个语言模型训练器的过程。</p><p>创建了一个语言模型学习器，它是一个用于训练语言模型的对象。</p><p><code>language_model_learner</code> 是 fastai 库中的一个函数，它接收以下参数：</p><ul><li><code>dls_lm</code>：一个 DataLoader 对象，它包含了训练和验证数据集。</li><li><code>AWD_LSTM</code>：模型架构，这里使用的是 AWD-LSTM 架构。</li><li><code>drop_mult</code>：一个浮点数，用于控制 dropout 层的比例。Dropout 是一种正则化技术，可以帮助防止模型过拟合。<code>drop_mult=0.3</code> 表示 dropout 比例为 30%。</li><li><code>metrics</code>：一个列表，包含了用于评估模型性能的指标。这里使用的是准确率（accuracy）和困惑度（Perplexity）。</li></ul><p><code>.to_fp16()</code> 是一个方法，它将模型的权重从 float32 转换为 float16，以节省内存和加速训练，但可能会稍微降低模型的精度。这种方法通常在 GPU 上训练大型模型时使用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>fit_one_cycle<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2e-2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406021629598.png" width="500"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">训练结果</div></center><h3 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h3><p>每个训练周期都需花费很长时间，所以将在训练过程中保存中间模型的结果。所以使用fit_one_cycle而不是fine_tine。language_model_learner在使用预训练模型时，会自动调用freeze，只训练嵌入层（模型中唯一包含随机初始化权重的部分）即，对于那些在我们的IMDb词汇表中但不在预训练模型词汇表中的词的嵌入。</p><p>保存</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'1epoch'</span><span class="token punctuation">)</span>
<span class="token comment"># Path('/home/sunqx/.fastai/data/imdb/models/1epoch.pth')</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>加载</p><p>要在另一台机器上加载模型，或者稍后继续训练，可以使用以下方法加载名为 <code>1epoch.pth</code> 的文件内容：</p><p>首先，确保模型架构与保存 <code>.pth</code> 文件时使用的架构相同。然后，可以使用 <code>torch.load()</code> 函数来加载状态字典（state_dict），并使用 <code>load_state_dict()</code> 方法将其应用到模型中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn <span class="token operator">=</span> learn<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'1epoch'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>一旦初始训练完成，就可以在解冻后继续微调模型了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>unfreeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
learn<span class="token punctuation">.</span>fit_one_cycle<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2e-3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406031204139.png" width="500"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">解冻后继续微调</div></center><p>保存除了最后一层的所有模型（不包括最后一层的模型叫做<strong>编码器</strong>），这将模型的激活值转换为词汇表中选择每个语义单元的概率。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>save_encoder<span class="token punctuation">(</span><span class="token string">'finetuned'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><code>编码器</code>：该模型不包括特定于任务的最终层。当应用于卷积神经网络时，该术语与“body”的含义大致相同，但“编码器”更倾向于在自然语言处理和生成式大模型中使用。</p><p>这就完成了微调语言模型。接下来可以用它和IMDb情感标签来微调分类器，然而在微调分类器之前可以尝试：使用模型生成随机评论。</p><h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h3><p>训练模型用来猜测句子中的下一个单词，可以用来写新评论：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">TEXT <span class="token operator">=</span> <span class="token string">"I liked this movie because"</span>
N_WORDS <span class="token operator">=</span> <span class="token number">40</span>
N_SENTENCES <span class="token operator">=</span> <span class="token number">2</span>
preds <span class="token operator">=</span> <span class="token punctuation">[</span>learn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> N_WORDS<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">)</span> 
         <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_SENTENCES<span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># i liked this movie because of the great acting and excellent direction . It 's a story about a married man who wants to be love and his wife . He lives with his dad , who has fallen into love with a</span>
<span class="token comment"># i liked this movie because it seemed like a typical Hong Kong action flick . There are two areas : Hong Kong and a lot of Hong Kong , plus most of the cinema is different than in</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>添加了一些随机性，根据模型返回的概率选择一个随机单词，因此两次不会获得完全相同的评论。</p><h3 id="创建分类器的数据加载器"><a href="#创建分类器的数据加载器" class="headerlink" title="创建分类器的数据加载器"></a>创建分类器的数据加载器</h3><p>从语言模型微调转向分类器微调。概括地说，语言模型预测文档的下一个单词，因此它不需要任何外部标签。然而，分类器预测一个外部标签——在IMDb的例子中，这个外部标签表示的是文档的情感。</p><p>分类数据块</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dls_clas <span class="token operator">=</span> DataBlock<span class="token punctuation">(</span>
    blocks<span class="token operator">=</span><span class="token punctuation">(</span>TextBlock<span class="token punctuation">.</span>from_folder<span class="token punctuation">(</span>path<span class="token punctuation">,</span> vocab<span class="token operator">=</span>dls_lm<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>CategoryBlock<span class="token punctuation">)</span><span class="token punctuation">,</span>
    get_y <span class="token operator">=</span> parent_label<span class="token punctuation">,</span>
    get_items<span class="token operator">=</span>partial<span class="token punctuation">(</span>get_text_files<span class="token punctuation">,</span> folders<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    splitter<span class="token operator">=</span>GrandparentSplitter<span class="token punctuation">(</span>valid_name<span class="token operator">=</span><span class="token string">'test'</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">.</span>dataloaders<span class="token punctuation">(</span>path<span class="token punctuation">,</span> path<span class="token operator">=</span>path<span class="token punctuation">,</span> bs<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token number">72</span><span class="token punctuation">)</span>

dls_clas<span class="token punctuation">.</span>show_batch<span class="token punctuation">(</span>max_n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406031208169.png" width="900"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">文本分类样本标签</div></center><p>从数据块的构建看，与之前两个重要的不同：</p><ul><li><code>TextBlock.from_floder</code>函数不再有<code>is_lm=True</code>参数，默认是<code>False</code>，告诉TextBlock，有常规的语义单元数据，而不是使用下一个语义单元作为标签。</li><li>传入了为语言模型微调创建的<code>vocab</code>，以确保使用相同的语义单元及索引之间的对应关系，否则，在微调语言模型中学习的嵌入对这个模型没有意义。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">将多个文档整理成小批次
nums_samp <span class="token operator">=</span> toks200<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>num<span class="token punctuation">)</span> <span class="token comment">#10个一批次</span>
nums_samp<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">)</span>
<span class="token comment"># (#10) [157,247,154,182,73,221,169,215,772,114]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>PyTorch的DataLoaders需要将一个批次中的所有数据项整理成单个张量，单个张量具有固定的形状（即在每个轴上都有特定的长度，所有数据项必须一致）。跟图像上的处理很相似，但是不能或者还没有尝试过对文本裁剪等。数据增强还没有在自然语言处理中得到很好的探索，所以也许在自然语言处理中也有机会使用裁剪！但是可以填充文本。</p><p>扩展文本，使它们大小相同。为此使用特殊的填充语义单元，这个语义单元将被模型忽略。此外，为了避免内存问题，并提高性能，将把长度大致相同的文本分批放在一起（对训练集进行一些排序）。其结果是，<strong>整理成一批的文件往往长度相似。不会将每批都填充为相同大小，而是使用每批中最大文档的大小作为目标大小。</strong>(idea：对图像做类似的处理，对不规则矩形图像尤其有用)</p><p>当使用<code>TextBlock</code>和<code>is_lm=False</code>时，数据块API会自动为我们完成排序和填充。（对于语言模型数据，没有这样的问题，因为我们首先将所有文档连接在一起，然后将它们分成大小相等的部分。）</p><p>创建一个模型对文本分类：训练分类器之前的最后一步是从我们微调的语言模型加载编码器。我们使用load_encoder而不是load，因为我们只有预训练的权重可用于编码器;如果加载了不完整的模型，load默认会引发异常：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn <span class="token operator">=</span> text_classifier_learner<span class="token punctuation">(</span>dls_clas<span class="token punctuation">,</span> AWD_LSTM<span class="token punctuation">,</span> drop_mult<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> 
                                metrics<span class="token operator">=</span>accuracy<span class="token punctuation">)</span><span class="token punctuation">.</span>to_fp16<span class="token punctuation">(</span><span class="token punctuation">)</span>
learn <span class="token operator">=</span> learn<span class="token punctuation">.</span>load_encoder<span class="token punctuation">(</span><span class="token string">'finetuned'</span><span class="token punctuation">)</span> <span class="token comment"># 从微调的语言模型加载编码器</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="微调分类模型"><a href="#微调分类模型" class="headerlink" title="微调分类模型"></a>微调分类模型</h3><p>用不同的学习率和逐渐解冻的方式训练。CV中经常一次性解冻所有模型，但对于NLP分类器，一次解冻几层会有不同的效果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>fit_one_cycle<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2e-2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406031216778.png" width="400"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">训练一个周期</div></center><p>只需一个时期，训练效果还不错！可以将<code>-2</code>传入<code>freeze_to</code>来冻结除最后两个参数组之外的所有参数组：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>freeze_to<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># 解冻最后两层</span>
learn<span class="token punctuation">.</span>fit_one_cycle<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">1e-2</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">2.6</span><span class="token operator">**</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1e-2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#学习率，它是一个范围，表示学习率从 1e-2/(2.6**4) 线性增加到 1e-2。这种设置通常用于训练深度神经网络，可以帮助模型更好地收敛。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406031228914.png" width="400"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">逐渐解冻</div></center><p>然后多结冻一点，继续训练：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>freeze_to<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
learn<span class="token punctuation">.</span>fit_one_cycle<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">5e-3</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">2.6</span><span class="token operator">**</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">5e-3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406031232984.png" width="400"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">逐渐解冻</div></center><p>最后解冻整个模型：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">learn<span class="token punctuation">.</span>unfreeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
learn<span class="token punctuation">.</span>fit_one_cycle<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">1e-3</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">2.6</span><span class="token operator">**</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1e-3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><center><img src="/themes/matery/source/medias/loading.gif" data-original="https://gitee.com/nusqx/picgo/raw/master/blog/202406031236676.png" width="400"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">完全解冻</div></center><p>对原文本进行翻转后，使用反向的文本进行训练另一个模型，并并计算这两个模型的预测平均值，准确率又有所提高。</p><p>使用预训练模型，我们可以构建一个功能强大的微调语言模型，既可以生成虚假评论，也可以帮助对虚假评论进行分类。这项技术也可能被用于恶意目的。</p><h2 id="虚假信息和语言模型"><a href="#虚假信息和语言模型" class="headerlink" title="虚假信息和语言模型"></a>虚假信息和语言模型</h2><p>随着生成算法的不断进步，分类或鉴别算法也需要不断地更新以保持有效性。这是一个动态平衡的过程，需要持续的研究和开发。</p><p>两种模型：能够生成文本的语言模型，以及判断评论正面或负面的分类器。构建一个先进的分类器通常涉及使用预训练的语言模型，将其微调到特定任务的语料库上，然后使用其编码器（encoder）部分配合一个新的头部（head）来进行分类。</p><blockquote><p><a target="_blank" rel="noopener" href="https://nbviewer.org/github/fastai/fastbook/blob/master/10_nlp.ipynb">https://nbviewer.org/github/fastai/fastbook/blob/master/10_nlp.ipynb</a></p></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="../../../../about" rel="external nofollow noreferrer">nusqx</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://nusqx.top">https://nusqx.top</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="../../../../about" target="_blank">nusqx</a> !</span></div></div><script async defer>document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="../../../../tags/nlp/"><span class="chip bg-color">nlp</span> </a><a href="../../../../tags/%E9%A2%84%E8%AE%AD%E7%BB%83/"><span class="chip bg-color">预训练</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="../../../../libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="../../../../libs/share/js/social-share.min.js"></script></div></div></div><div id="reward"><a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a><div id="rewardModal" class="modal"><div class="modal-content"><a class="close modal-close"><i class="fas fa-times"></i></a><h4 class="reward-title">你的赏识是我前进的动力</h4><div class="reward-content"><div class="reward-tabs"><ul class="tabs row"><li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li><li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li></ul><div id="alipay"><img src="/themes/matery/source/medias/loading.gif" data-original="../../../../medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码"></div><div id="wechat"><img src="/themes/matery/source/medias/loading.gif" data-original="../../../../medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码"></div></div></div></div></div></div><script>$(function(){$(".tabs").tabs()})</script></div></div><style>.valine-card{margin:1.5rem auto}.valine-card .card-content{padding:20px 20px 5px 20px}#vcomments textarea{box-sizing:border-box;background:url(../../../medias/comment_bg.png) 100% 100% no-repeat}#vcomments p{margin:2px 2px 10px;font-size:1.05rem;line-height:1.78rem}#vcomments blockquote p{text-indent:.2rem}#vcomments a{padding:0 2px;color:#4cbf30;font-weight:500;text-decoration:none}#vcomments img{max-width:100%;height:auto;cursor:pointer}#vcomments ol li{list-style-type:decimal}#vcomments ol,ul{display:block;padding-left:2em;word-spacing:.05rem}#vcomments ul li,ol li{display:list-item;line-height:1.8rem;font-size:1rem}#vcomments ul li{list-style-type:disc}#vcomments ul ul li{list-style-type:circle}#vcomments table,td,th{padding:12px 13px;border:1px solid #dfe2e5}#vcomments table,td,th{border:0}table tr:nth-child(2n),thead{background-color:#fafafa}#vcomments table th{background-color:#f2f2f2;min-width:80px}#vcomments table td{min-width:80px}#vcomments h1{font-size:1.85rem;font-weight:700;line-height:2.2rem}#vcomments h2{font-size:1.65rem;font-weight:700;line-height:1.9rem}#vcomments h3{font-size:1.45rem;font-weight:700;line-height:1.7rem}#vcomments h4{font-size:1.25rem;font-weight:700;line-height:1.5rem}#vcomments h5{font-size:1.1rem;font-weight:700;line-height:1.4rem}#vcomments h6{font-size:1rem;line-height:1.3rem}#vcomments p{font-size:1rem;line-height:1.5rem}#vcomments hr{margin:12px 0;border:0;border-top:1px solid #ccc}#vcomments blockquote{margin:15px 0;border-left:5px solid #42b983;padding:1rem .8rem .3rem .8rem;color:#666;background-color:rgba(66,185,131,.1)}#vcomments pre{font-family:monospace,monospace;padding:1.2em;margin:.5em 0;background:#272822;overflow:auto;border-radius:.3em;tab-size:4}#vcomments code{font-family:monospace,monospace;padding:1px 3px;font-size:.92rem;color:#e96900;background-color:#f8f8f8;border-radius:2px}#vcomments pre code{font-family:monospace,monospace;padding:0;color:#e8eaf6;background-color:#272822}#vcomments pre[class*=language-]{padding:1.2em;margin:.5em 0}#vcomments code[class*=language-],pre[class*=language-]{color:#e8eaf6}#vcomments [type=checkbox]:not(:checked),[type=checkbox]:checked{position:inherit;margin-left:-1.3rem;margin-right:.4rem;margin-top:-1px;vertical-align:middle;left:unset;visibility:visible}#vcomments b,strong{font-weight:700}#vcomments dfn{font-style:italic}#vcomments small{font-size:85%}#vcomments cite{font-style:normal}#vcomments mark{background-color:#fcf8e3;padding:.2em}#vcomments table,td,th{padding:12px 13px;border:1px solid #dfe2e5}table tr:nth-child(2n),thead{background-color:#fafafa}#vcomments table th{background-color:#f2f2f2;min-width:80px}#vcomments table td{min-width:80px}#vcomments [type=checkbox]:not(:checked),[type=checkbox]:checked{position:inherit;margin-left:-1.3rem;margin-right:.4rem;margin-top:-1px;vertical-align:middle;left:unset;visibility:visible}</style><div class="card valine-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="vcomments" class="card-content" style="display:grid"></div></div><script src="../../../../libs/valine/av-min.js"></script><script src="../../../../libs/valine/Valine.min.js"></script><script>new Valine({el:"#vcomments",appId:"GOX0XlpzLZat5ANucw5j9zjl-gzGzoHsz",appKey:"fha9hT9W6BxJDz7eBYQEPBvc",serverURLs:"",notify:!0,verify:!0,visitor:!1,avatar:"wavatar",pageSize:"10",lang:"zh-cn",placeholder:"What do you say?"})</script><div id="to_comment" class="comment-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#vcomments" title="直达评论"><i class="fas fa-comments"></i></a></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="../../05/springboot-3/"><div class="card-image"><img src="/themes/matery/source/medias/loading.gif" data-original="../../../../medias/featureimages/9.jpg" class="responsive-img" alt="SpringBoot项目部署与多环境开发"> <span class="card-title">SpringBoot项目部署与多环境开发</span></div></a><div class="card-content article-content"><div class="summary block-with-text">SpringBoot项目部署、多环境开发</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-06-05 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="../../../../categories/SpringBoot/" class="post-category">SpringBoot</a></span></div></div><div class="card-action article-tags"><a href="../../../../tags/SpringBoot/"><span class="chip bg-color">SpringBoot</span> </a><a href="../../../../tags/Java/"><span class="chip bg-color">Java</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="../imageclassifer/"><div class="card-image"><img src="/themes/matery/source/medias/loading.gif" data-original="../../../../medias/featureimages/21.jpg" class="responsive-img" alt="图像分类器"> <span class="card-title">图像分类器</span></div></a><div class="card-content article-content"><div class="summary block-with-text">预处理、交叉熵损失、微调</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-06-03 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="../../../../tags/fastai/"><span class="chip bg-color">fastai</span> </a><a href="../../../../tags/cv/"><span class="chip bg-color">cv</span></a></div></div></div></div></article></div><script>$("#articleContent").on("copy",function(e){var n,t,o,i;void 0!==window.getSelection&&((""+(n=window.getSelection())).length<Number.parseInt("240")||(t=document.getElementsByTagName("body")[0],(o=document.createElement("div")).style.position="absolute",o.style.left="-99999px",t.appendChild(o),o.appendChild(n.getRangeAt(0).cloneContents()),"PRE"!==n.getRangeAt(0).commonAncestorContainer.nodeName&&"CODE"!==n.getRangeAt(0).commonAncestorContainer.nodeName||(o.innerHTML="<pre>"+o.innerHTML+"</pre>"),i=document.location.href,o.innerHTML+='<br />来源: SQX BLOG<br />文章作者: NUS QX<br />文章链接: <a href="'+i+'">'+i+"</a><br />本文章著作权归作者所有，任何形式的转载都请注明出处。",n.selectAllChildren(o),window.setTimeout(function(){t.removeChild(o)},200)))})</script><script type="text/javascript" src="../../../../libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="../../../../libs/prism/prism.min.js"></script><script type="text/javascript" src="../../../../libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="../../../../libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="../../../../libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="../../../../libs/tocbot/tocbot.min.js"></script><script>$(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });</script></main><script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]]}})</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2018-2024</span> <a href="../../../../about" target="_blank">NUS QX</a><br><span id="sitetime"></span><span class="my-face"></span><br>&nbsp;|&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">117.2k</span> <span id="busuanzi_container_site_pv" style="display:none"></span> &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span> <span id="busuanzi_container_site_uv" style="display:none"></span> 次&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;访客人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span> 人 <span id="busuanzi_value_site_uv" class="white-color"></span></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/nusqx" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:1976490928@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fa-solid fa-envelope"></i> </a><a href="https://gitee.com/nusqx" class="tooltipped" target="_blank" data-tooltip="访问我的Gitee: https://gitee.com/nusqx" data-position="top" data-delay="50"><i class="fa-brands fa-square-git"></i> </a><a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1976490928" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1976490928" data-position="top" data-delay="50"><i class="fab fa-qq"></i></a></div></div></footer><div class="progress-bar"></div><script language="javascript">function siteTime(){window.setTimeout("siteTime()",1e3);var e=36e5,t=24*e,o=new Date,i=o.getFullYear(),a=o.getMonth()+1,n=o.getDate(),r=o.getHours(),l=o.getMinutes(),s=o.getSeconds(),M=Date.UTC(2018,9,24,0,0,0),g=Date.UTC(i,a,n,r,l,s)-M,m=Math.floor(g/31536e6),T=Math.floor(g/t-365*m),f=Math.floor((g-(365*m+T)*t)/e),h=Math.floor((g-(365*m+T)*t-f*e)/6e4),u=Math.floor((g-(365*m+T)*t-f*e-6e4*h)/1e3);document.getElementById("sitetime").innerHTML="本站已运行 "+m+" 年 "+T+" 天 "+f+" 小时 "+h+" 分钟 "+u+" 秒"}siteTime()</script><script>$(document).ready(function(){var e=setInterval(function(){"none"!=document.getElementById("busuanzi_container_site_pv").style.display&&($("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html())+n),clearInterval(e));"none"!=$("#busuanzi_container_site_pv").css("display")&&($("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html())+t),clearInterval(e))},50),n=8e4,t=2e4})</script><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script type="text/javascript">$(function(){!function(t,s,i){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var e=$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),n=document.getElementById(s),r=document.getElementById(i);n.addEventListener("input",function(){var f='<ul class="search-result-list">',m=this.value.trim().toLowerCase().split(/[\s\-]+/);r.innerHTML="",this.value.trim().length<=0||(e.forEach(function(t){var n,e,r,s,i,l=!0,a=t.title.trim().toLowerCase(),c=t.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),u=0===(u=t.url).indexOf("/")?t.url:"/"+u,o=-1,h=-1;""!==a&&""!==c&&m.forEach(function(t,e){n=a.indexOf(t),o=c.indexOf(t),n<0&&o<0?l=!1:(o<0&&(o=0),0===e&&(h=o))}),l&&(f+="<li><a href='"+u+"' class='search-result-title'>"+a+"</a>",e=t.content.trim().replace(/<[^>]+>/g,""),0<=h&&(s=h+80,(r=h-20)<0&&(r=0),0===r&&(s=100),s>e.length&&(s=e.length),i=e.substr(r,s),m.forEach(function(t){var e=new RegExp(t,"gi");i=i.replace(e,'<em class="search-keyword">'+t+"</em>")}),f+='<p class="search-result">'+i+"...</p>"),f+="</li>")}),f+="</ul>",r.innerHTML=f)})}})}("../../../../search.xml","searchInput","searchResult")})</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout(function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout(function(){$(".Cuteen_DarkSky").fadeOut(1e3,function(){$(this).remove()})},2e3)})}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="../../../../libs/materialize/materialize.min.js"></script><script src="../../../../libs/masonry/masonry.pkgd.min.js"></script><script src="../../../../libs/aos/aos.js"></script><script src="../../../../libs/scrollprogress/scrollProgress.min.js"></script><script src="../../../../libs/lightGallery/js/lightgallery-all.min.js"></script><script src="../../../../js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="../../../../libs/others/clicklove.js" async></script><script async src="../../../../libs/others/busuanzi.pure.mini.js"></script><script type="text/javascript" src="../../../../libs/background/ribbon-dynamic.js" async></script><script src="../../../../libs/instantpage/instantpage.js" type="module"></script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:1,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a=c[o],i=function(){c=c.filter(function(t){return a!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(a)};(t=a).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),i()):(e=new Image,n=t.getAttribute("data-original"),e.onload=function(){t.src=n,t.removeAttribute("data-original"),i()},t.src!==n&&(e.src=n))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this)</script><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>